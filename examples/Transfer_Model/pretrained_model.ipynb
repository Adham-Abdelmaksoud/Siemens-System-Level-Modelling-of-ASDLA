{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models in deep learning are trained on large datasets. Here are some popular pretrained models:\n",
    "\n",
    "1. **VGG (Visual Geometry Group) Models:**\n",
    "   - VGG16\n",
    "   - VGG19\n",
    "\n",
    "2. **ResNet (Residual Networks) Models:**\n",
    "   - ResNet-18\n",
    "   - ResNet-34\n",
    "   - ResNet-50\n",
    "   - ResNet-101\n",
    "   - ResNet-152\n",
    "\n",
    "3. **Inception Models:**\n",
    "   - InceptionV3\n",
    "   - InceptionResNetV2\n",
    "\n",
    "4. **MobileNet Models:**\n",
    "   - MobileNetV1\n",
    "   - MobileNetV2\n",
    "   - MobileNetV3\n",
    "\n",
    "5. **DenseNet Models:**\n",
    "   - DenseNet-121\n",
    "   - DenseNet-169\n",
    "   - DenseNet-201\n",
    "\n",
    "6. **EfficientNet Models:**\n",
    "   - EfficientNetB0, B1, B2, B3, B4, B5, B6, B7\n",
    "\n",
    "7. **Xception (Extreme Inception) Model**\n",
    "\n",
    "8. **ResNeXt Models:**\n",
    "   - ResNeXt-50\n",
    "   - ResNeXt-101\n",
    "\n",
    "9. **SqueezeNet Model**\n",
    "\n",
    "10. **NASNet (Neural Architecture Search Network) Models:**\n",
    "    - NASNetLarge\n",
    "    - NASNetMobile\n",
    "\n",
    "11. **SENet (Squeeze-and-Excitation Networks) Models:**\n",
    "    - SENet-154\n",
    "    - SENet-50\n",
    "\n",
    "12. **ShuffleNet Models:**\n",
    "    - ShuffleNetV1\n",
    "    - ShuffleNetV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosen Models: \"a5tarthom according to my preferences\"\n",
    "\n",
    "VGG19\n",
    "ResNet-50\n",
    "InceptionV3\n",
    "MobileNetV3 \"MobileNetV3 is not directly available in torchvision\"\n",
    "DenseNet-169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Define a class for VGG19 without the last layer\n",
    "class VGG19WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(VGG19WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained VGG19 model from torchvision\n",
    "        vgg19 = models.vgg19(weights=\"DEFAULT\")\n",
    "        print(nn.Sequential(*list(vgg19.children())[:-1]))\n",
    "        # Extract the feature layers (convolutional and pooling layers)\n",
    "        self.features = vgg19.features\n",
    "        # print(vgg19)\n",
    "        # Global average pooling layer\n",
    "        self.avgpool = vgg19.avgpool\n",
    "        # print(self.avgpool)\n",
    "        # Extract the classifier layers, excluding the last fully connected layer\n",
    "        self.classifier = nn.Sequential(*list(vgg19.classifier.children())[:-1])\n",
    "        # print(vgg19.classifier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through the classifier layers\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Function to add a custom last layer to the base model\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Identify the index of the last linear layer in the classifier\n",
    "    last_layer_index = None\n",
    "    for i, layer in enumerate(reversed(base_model.classifier)):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            last_layer_index = len(base_model.classifier) - 1 - i\n",
    "            break\n",
    "\n",
    "    # Remove the dropout layers if present\n",
    "    if last_layer_index is not None:\n",
    "        base_model.classifier = base_model.classifier[:last_layer_index + 1]\n",
    "\n",
    "    # Add a linear layer (fully connected) with the specified number of classes\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.classifier[-1].out_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage: BUT IN OUR CASE WE WILL USE METHOD WHICH IS IN THE TOOL\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create VGG19 base model without the last layer\n",
    "base_model = VGG19WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet50WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet-50 model from torchvision\n",
    "        resnet50 = models.resnet50(weights=\"DEFAULT\")\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Find the last AdaptiveAvgPool2d layer\n",
    "    last_pooling_layer = None\n",
    "    for layer in reversed(base_model.features):\n",
    "        if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "            last_pooling_layer = layer\n",
    "            break\n",
    "\n",
    "    if last_pooling_layer is not None:\n",
    "        # Determine the number of output features from the last pooling layer\n",
    "        num_features = last_pooling_layer.output_size[0] * last_pooling_layer.output_size[1]\n",
    "    else:\n",
    "        # If AdaptiveAvgPool2d layer is not found, use the number of in_features of the last layer in base_model.features\n",
    "        num_features = base_model.features[-1][0].in_features\n",
    "\n",
    "    # Add a linear layer (fully connected) with the specified number of classes\n",
    "    last_layer = nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create ResNet-50 base model without the last layer\n",
    "base_model = ResNet50WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class InceptionV3WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(InceptionV3WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained InceptionV3 model from torchvision\n",
    "        inception_v3 = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(inception_v3.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.features[-1][-1].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create InceptionV3 base model without the last layer\n",
    "base_model = InceptionV3WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet-169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class DenseNet169WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(DenseNet169WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained DenseNet-169 model from torchvision\n",
    "        densenet169 = models.densenet169(pretrained=True)\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = densenet169.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.features[-1].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create DenseNet-169 base model without the last layer\n",
    "base_model = DenseNet169WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LenNet \"Omar's Recommendation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BasicConv2d(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (2): BasicConv2d(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicConv2d(\n",
      "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (5): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (8): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (14): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): Inception(\n",
      "    (branch1): BasicConv2d(\n",
      "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): BasicConv2d(\n",
      "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "      (1): BasicConv2d(\n",
      "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (17): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Dropout' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Add a custom last layer based on your task\u001b[39;00m\n\u001b[0;32m    120\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Modify based on your classification task\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43madd_custom_last_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Define your loss function and optimizer\u001b[39;00m\n\u001b[0;32m    124\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[27], line 30\u001b[0m, in \u001b[0;36madd_custom_last_layer\u001b[1;34m(base_model, num_classes)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_custom_last_layer\u001b[39m(base_model, num_classes):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Access the output channels of the last layer (Inception module)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     last_module \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Check if the last module is an inception module\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_module, nn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mModuleList):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Dropout' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class LeNetWithoutLastLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetWithoutLastLayer, self).__init__()\n",
    "        self.lenet=models.googlenet(weights=\"DEFAULT\")\n",
    "        # LeNet architecture: conv1 -> relu -> pool1 -> conv2 -> relu -> pool2\n",
    "        self.features=  nn.Sequential(*list(self.lenet.children())[:-1])\n",
    "        print(self.features)\n",
    "        # = nn.Sequential(\n",
    "        #     nn.Conv2d(1, 6, kernel_size=5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.Conv2d(6, 16, kernel_size=5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Access the output channels of the last layer (Inception module)\n",
    "    last_module = base_model.features[-1][-1]\n",
    "    \n",
    "    # Check if the last module is an inception module\n",
    "    if isinstance(last_module, nn.modules.container.ModuleList):\n",
    "        last_layer_out_channels = last_module[-1].out_channels\n",
    "    else:\n",
    "        last_layer_out_channels = last_module.out_channels\n",
    "\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=last_layer_out_channels, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create LeNet base model without the last layer\n",
    "base_model = LeNetWithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
