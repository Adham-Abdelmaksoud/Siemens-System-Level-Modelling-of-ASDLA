{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models in deep learning are trained on large datasets. Here are some popular pretrained models:\n",
    "\n",
    "1. **VGG (Visual Geometry Group) Models:**\n",
    "   - VGG16\n",
    "   - VGG19\n",
    "\n",
    "2. **ResNet (Residual Networks) Models:**\n",
    "   - ResNet-18\n",
    "   - ResNet-34\n",
    "   - ResNet-50\n",
    "   - ResNet-101\n",
    "   - ResNet-152\n",
    "\n",
    "3. **Inception Models:**\n",
    "   - InceptionV3\n",
    "   - InceptionResNetV2\n",
    "\n",
    "4. **MobileNet Models:**\n",
    "   - MobileNetV1\n",
    "   - MobileNetV2\n",
    "   - MobileNetV3\n",
    "\n",
    "5. **DenseNet Models:**\n",
    "   - DenseNet-121\n",
    "   - DenseNet-169\n",
    "   - DenseNet-201\n",
    "\n",
    "6. **EfficientNet Models:**\n",
    "   - EfficientNetB0, B1, B2, B3, B4, B5, B6, B7\n",
    "\n",
    "7. **Xception (Extreme Inception) Model**\n",
    "\n",
    "8. **ResNeXt Models:**\n",
    "   - ResNeXt-50\n",
    "   - ResNeXt-101\n",
    "\n",
    "9. **SqueezeNet Model**\n",
    "\n",
    "10. **NASNet (Neural Architecture Search Network) Models:**\n",
    "    - NASNetLarge\n",
    "    - NASNetMobile\n",
    "\n",
    "11. **SENet (Squeeze-and-Excitation Networks) Models:**\n",
    "    - SENet-154\n",
    "    - SENet-50\n",
    "\n",
    "12. **ShuffleNet Models:**\n",
    "    - ShuffleNetV1\n",
    "    - ShuffleNetV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosen Models: \"a5tarthom according to my preferences\"\n",
    "\n",
    "VGG19\n",
    "ResNet-50\n",
    "InceptionV3\n",
    "MobileNetV3 \"MobileNetV3 is not directly available in torchvision\"\n",
    "DenseNet-169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Define a class for VGG19 without the last layer\n",
    "class VGG19WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(VGG19WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained VGG19 model from torchvision\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "        # Extract the feature layers (convolutional and pooling layers)\n",
    "        self.features = vgg19.features\n",
    "\n",
    "        # Global average pooling layer\n",
    "        self.avgpool = vgg19.avgpool\n",
    "\n",
    "        # Extract the classifier layers, excluding the last fully connected layer\n",
    "        self.classifier = nn.Sequential(*list(vgg19.classifier.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through the classifier layers\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Function to add a custom last layer to the base model\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add a linear layer (fully connected) with the specified number of classes\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.classifier[-1].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage: BUT IN OUR CASE WE WILL USE METHOD WHICH IS IN THE TOOL\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create VGG19 base model without the last layer\n",
    "base_model = VGG19WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ResNet50WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet50WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet-50 model from torchvision\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.features[-1][0].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create ResNet-50 base model without the last layer\n",
    "base_model = ResNet50WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class InceptionV3WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(InceptionV3WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained InceptionV3 model from torchvision\n",
    "        inception_v3 = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(inception_v3.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.features[-1][-1].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create InceptionV3 base model without the last layer\n",
    "base_model = InceptionV3WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet-169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class DenseNet169WithoutLastLayer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(DenseNet169WithoutLastLayer, self).__init__()\n",
    "\n",
    "        # Load the pretrained DenseNet-169 model from torchvision\n",
    "        densenet169 = models.densenet169(pretrained=True)\n",
    "\n",
    "        # Extract all layers except the last fully connected layer\n",
    "        self.features = densenet169.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.features[-1].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create DenseNet-169 base model without the last layer\n",
    "base_model = DenseNet169WithoutLastLayer()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV3 \"MobileNetV3 is not directly available in torchvision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MobileNetV3Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, stride, expansion_factor, se_ratio\n",
    "    ):\n",
    "        super(MobileNetV3Block, self).__init__()\n",
    "        hidden_dim = int(in_channels * expansion_factor)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act1 = nn.ReLU6(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden_dim,\n",
    "            hidden_dim,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            kernel_size // 2,\n",
    "            groups=hidden_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act2 = nn.ReLU6(inplace=True)\n",
    "\n",
    "        # Squeeze-and-Excitation layer\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(hidden_dim, int(hidden_dim * se_ratio), 1),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(int(hidden_dim * se_ratio), hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.act3 = nn.ReLU6(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        se_weights = self.se(x)\n",
    "        x = x * se_weights\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Skip connection\n",
    "        x += identity\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes=1000, input_size=224, width_multiplier=1.0, se_ratio=0.25\n",
    "    ):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        channels = [16, 24, 40, 80, 112, 160, 960]\n",
    "        last_channels = int(1280 * width_multiplier) if width_multiplier > 1.0 else 1280\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, int(channels[0] * width_multiplier), 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(int(channels[0] * width_multiplier)),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            MobileNetV3Block(\n",
    "                int(channels[0] * width_multiplier),\n",
    "                int(channels[1] * width_multiplier),\n",
    "                3,\n",
    "                2,\n",
    "                1,\n",
    "                se_ratio,\n",
    "            ),\n",
    "            MobileNetV3Block(\n",
    "                int(channels[1] * width_multiplier),\n",
    "                int(channels[2] * width_multiplier),\n",
    "                3,\n",
    "                2,\n",
    "                6,\n",
    "                se_ratio,\n",
    "            ),\n",
    "            MobileNetV3Block(\n",
    "                int(channels[2] * width_multiplier),\n",
    "                int(channels[3] * width_multiplier),\n",
    "                3,\n",
    "                2,\n",
    "                6,\n",
    "                se_ratio,\n",
    "            ),\n",
    "            MobileNetV3Block(\n",
    "                int(channels[3] * width_multiplier),\n",
    "                int(channels[4] * width_multiplier),\n",
    "                3,\n",
    "                1,\n",
    "                6,\n",
    "                se_ratio,\n",
    "            ),\n",
    "            MobileNetV3Block(\n",
    "                int(channels[4] * width_multiplier),\n",
    "                int(channels[5] * width_multiplier),\n",
    "                3,\n",
    "                1,\n",
    "                6,\n",
    "                se_ratio,\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                int(channels[5] * width_multiplier), last_channels, 1, 1, 0, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_custom_last_layer(base_model, num_classes):\n",
    "    # Add your custom last layer based on the task (e.g., classification)\n",
    "    last_layer = nn.Linear(\n",
    "        in_features=base_model.classifier[0].in_features, out_features=num_classes\n",
    "    )\n",
    "\n",
    "    # Create a new model by appending the custom last layer to the base model\n",
    "    model = nn.Sequential(base_model, last_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cuda\"\n",
    "):\n",
    "    # Move the model to the specified device (e.g., GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables for calculating accuracy and loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient computation during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation dataset\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move inputs and labels to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch-wise statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# (Assuming you have train_loader and val_loader for your dataset)\n",
    "\n",
    "# Create MobileNetV3 base model without the last layer\n",
    "base_model = MobileNetV3()\n",
    "\n",
    "# Add a custom last layer based on your task\n",
    "num_classes = 10  # Modify based on your classification task\n",
    "model = add_custom_last_layer(base_model, num_classes)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
