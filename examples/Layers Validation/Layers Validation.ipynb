{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a standalone custom layer and its testing, you can create and test it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 5])\n",
      "Output values: tensor([[-0.4971, -0.4220, -0.6015, -0.5429, -0.1610],\n",
      "        [ 0.4513,  0.0354, -0.2019, -0.4823, -0.0701],\n",
      "        [ 0.6071,  0.3061,  0.7269,  0.7913,  0.5538]], grad_fn=<SinBackward0>)\n",
      "Custom layer weights: Parameter containing:\n",
      "tensor([[0.0220, 0.4254, 0.5066, 0.9658, 0.8950],\n",
      "        [0.5781, 0.7742, 0.3002, 0.1466, 0.9577],\n",
      "        [0.8081, 0.0510, 0.7395, 0.4055, 0.3747],\n",
      "        [0.2891, 0.3215, 0.1348, 0.5786, 0.6693],\n",
      "        [0.3701, 0.5491, 0.6556, 0.8324, 0.5624],\n",
      "        [0.8002, 0.9016, 0.6168, 0.2295, 0.5702],\n",
      "        [0.1859, 0.2443, 0.7709, 0.9012, 0.2784],\n",
      "        [0.2160, 0.3964, 0.6990, 0.7154, 0.1217],\n",
      "        [0.6104, 0.7239, 0.3413, 0.5432, 0.3181],\n",
      "        [0.9878, 0.8646, 0.7643, 0.7098, 0.5338]], requires_grad=True)\n",
      "Custom layer bias: Parameter containing:\n",
      "tensor([-0.0019, -0.0016, -0.0021, -0.0029, -0.0024], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Custom non-linear operation\n",
    "        return torch.sin(x.mm(self.weight) + self.bias)\n",
    "\n",
    "\n",
    "# Example usage to test the custom layer\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "batch_size = 3\n",
    "\n",
    "# Create an instance of your custom layer\n",
    "custom_layer = CustomLayer(input_size, output_size)\n",
    "\n",
    "# Move the custom layer and input data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_layer.to(device)\n",
    "input_data = torch.rand((batch_size, input_size)).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = custom_layer(input_data)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output values:\", output)\n",
    "\n",
    "# Backward pass (gradient computation)\n",
    "loss_function = nn.MSELoss()\n",
    "target = torch.rand_like(output).to(device)\n",
    "loss = loss_function(output, target)\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer update (optional)\n",
    "optimizer = optim.SGD(custom_layer.parameters(), lr=0.01)\n",
    "optimizer.step()\n",
    "\n",
    "# Check if the parameters of the custom layer have been updated\n",
    "print(\"Custom layer weights:\", custom_layer.weight)\n",
    "print(\"Custom layer bias:\", custom_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code includes a simple training loop with a mean squared error loss and stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.41681012511253357\n",
      "Epoch 100, Loss: 0.27911481261253357\n",
      "Epoch 200, Loss: 0.23044569790363312\n",
      "Epoch 300, Loss: 0.21271133422851562\n",
      "Epoch 400, Loss: 0.20513923466205597\n",
      "Epoch 500, Loss: 0.20134583115577698\n",
      "Epoch 600, Loss: 0.19897450506687164\n",
      "Epoch 700, Loss: 0.19714219868183136\n",
      "Epoch 800, Loss: 0.19552579522132874\n",
      "Epoch 900, Loss: 0.19400539994239807\n",
      "Custom layer weights: Parameter containing:\n",
      "tensor([[0.1152, 0.1220, 0.1851, 0.4294, 0.8466],\n",
      "        [0.1535, 0.5954, 0.6350, 0.5144, 0.2337],\n",
      "        [0.6109, 0.4297, 0.3304, 0.2142, 0.1500],\n",
      "        [0.6589, 0.1326, 0.4990, 0.3534, 0.4469],\n",
      "        [0.4007, 0.7092, 0.6476, 0.1486, 0.6996],\n",
      "        [0.8808, 0.7338, 0.1925, 0.7481, 0.0036],\n",
      "        [0.8964, 0.4242, 0.7540, 0.5882, 0.4429],\n",
      "        [0.2343, 0.5844, 0.6002, 0.0583, 0.6616],\n",
      "        [0.6210, 0.5530, 0.4327, 0.8173, 0.2986],\n",
      "        [0.5848, 0.5522, 0.1778, 0.6297, 0.1636]], requires_grad=True)\n",
      "Custom layer bias: Parameter containing:\n",
      "tensor([-0.1360, -0.2908,  0.2538,  0.0235,  0.0320], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Custom non-linear operation\n",
    "        return torch.sin(x.mm(self.weight) + self.bias)\n",
    "\n",
    "\n",
    "# Example usage to train the custom layer\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Create an instance of your custom layer\n",
    "custom_layer = CustomLayer(input_size, output_size)\n",
    "\n",
    "# Move the custom layer and input data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_layer.to(device)\n",
    "input_data = torch.rand((batch_size, input_size)).to(device)\n",
    "target = torch.rand((batch_size, output_size)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(custom_layer.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = custom_layer(input_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_function(output, target)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Check the final parameters of the custom layer\n",
    "print(\"Custom layer weights:\", custom_layer.weight)\n",
    "print(\"Custom layer bias:\", custom_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a custom neural network layer using PyTorch and demonstrates how to train it. Let's break down the code step by step:\n",
    "\n",
    "1. **CustomLayer class definition:**\n",
    "   - The `CustomLayer` class is a subclass of `nn.Module`, the base class for all PyTorch neural network modules.\n",
    "   - It has an `__init__` method that initializes the layer with randomly initialized weights and zero biases. The weights and biases are stored as `nn.Parameter` objects, making them trainable during the optimization process.\n",
    "   - The `forward` method defines the forward pass of the layer, applying a custom non-linear operation to the input `x`. In this case, it computes the sine of the matrix multiplication of `x` with the weights and adds the bias.\n",
    "\n",
    "2. **Example usage to train the custom layer:**\n",
    "   - It specifies the input size (`input_size`), output size (`output_size`), batch size (`batch_size`), learning rate (`learning_rate`), and the number of epochs (`epochs`).\n",
    "   - Creates an instance of the `CustomLayer` with the specified input and output sizes.\n",
    "   - Checks whether a GPU is available and moves the custom layer and input data to the GPU if possible.\n",
    "   - Defines a mean squared error loss function (`nn.MSELoss`) and the stochastic gradient descent optimizer (`optim.SGD`) to update the parameters during training.\n",
    "\n",
    "3. **Training loop:**\n",
    "   - Iterates through the specified number of epochs.\n",
    "   - Performs a forward pass through the custom layer.\n",
    "   - Computes the mean squared error loss between the predicted output and the target.\n",
    "   - Performs a backward pass to compute gradients and updates the model parameters using stochastic gradient descent.\n",
    "\n",
    "4. **Prints the loss every 100 epochs during training.**\n",
    "\n",
    "5. **After training, it prints the final parameters of the custom layer (weights and biases).**\n",
    "\n",
    "In summary, this code demonstrates the creation and training of a custom neural network layer using PyTorch, where the layer applies a non-linear operation (sine function) to the input data during the forward pass. The training is performed using mean squared error loss and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code now includes additional checks for input validation, parameter initialization, and device compatibility\n",
    "and checks for unexpected forward pass output, invalid loss values, and NaN values in model parameters during each iteration of the training loop. If any of these issues are detected, warning messages will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2725360691547394\n",
      "Epoch 100, Loss: 0.23698008060455322\n",
      "Epoch 200, Loss: 0.2238324135541916\n",
      "Epoch 300, Loss: 0.21788565814495087\n",
      "Epoch 400, Loss: 0.2145886868238449\n",
      "Epoch 500, Loss: 0.21240290999412537\n",
      "Epoch 600, Loss: 0.21073603630065918\n",
      "Epoch 700, Loss: 0.20933374762535095\n",
      "Epoch 800, Loss: 0.20807668566703796\n",
      "Epoch 900, Loss: 0.20690453052520752\n",
      "Custom layer weights: Parameter containing:\n",
      "tensor([[ 0.8275,  0.7250,  0.2966,  0.7577,  0.5092],\n",
      "        [ 0.4686,  0.8673,  0.0864,  0.8612,  0.2449],\n",
      "        [ 0.4059,  0.6153,  0.2913,  0.7620,  0.6555],\n",
      "        [ 0.4160,  0.2468,  0.4900, -0.0017,  0.6629],\n",
      "        [ 0.8212,  0.3362,  0.8577,  0.5394,  0.4457],\n",
      "        [ 0.2779,  0.3074,  0.2047,  0.3181,  0.6938],\n",
      "        [ 0.6295,  0.2011,  0.3845,  0.5593,  0.1593],\n",
      "        [ 0.3796,  0.3657,  0.5580, -0.0296,  0.6510],\n",
      "        [ 0.2780,  0.7183,  0.6741,  0.3820,  0.0278],\n",
      "        [-0.0870,  0.1265,  0.6052,  0.3348,  0.0417]], requires_grad=True)\n",
      "Custom layer bias: Parameter containing:\n",
      "tensor([-0.0441,  0.0810,  0.0788, -0.0155, -0.1061], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "\n",
    "        # Input validation\n",
    "        if (\n",
    "            not isinstance(input_size, int)\n",
    "            or not isinstance(output_size, int)\n",
    "            or input_size <= 0\n",
    "            or output_size <= 0\n",
    "        ):\n",
    "            raise ValueError(\"Invalid input or output size.\")\n",
    "\n",
    "        # Parameter initialization\n",
    "        self.weight = nn.Parameter(torch.rand(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "        # Check parameter shapes\n",
    "        if self.weight.size() != (input_size, output_size) or self.bias.size() != (\n",
    "            output_size,\n",
    "        ):\n",
    "            print(\"Warning: Incorrect parameter initialization.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Custom non-linear operation\n",
    "        return torch.sin(x.mm(self.weight) + self.bias)\n",
    "\n",
    "\n",
    "# Example usage to train the custom layer\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Create an instance of your custom layer\n",
    "custom_layer = CustomLayer(input_size, output_size)\n",
    "\n",
    "# Move the custom layer and input data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_layer.to(device)\n",
    "input_data = torch.rand((batch_size, input_size)).to(device)\n",
    "target = torch.rand((batch_size, output_size)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(custom_layer.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = custom_layer(input_data)\n",
    "\n",
    "    # Additional checks\n",
    "    if not torch.is_tensor(output) or output.size() != (batch_size, output_size):\n",
    "        print(\"Warning: Unexpected forward pass output.\")\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_function(output, target)\n",
    "\n",
    "    # Additional checks\n",
    "    if not torch.is_tensor(loss) or loss.dim() != 0:\n",
    "        print(\"Warning: Invalid loss value.\")\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Additional checks\n",
    "    optimizer.step()\n",
    "    if any(torch.isnan(param).any() for param in custom_layer.parameters()):\n",
    "        print(\"Warning: NaN values detected in model parameters.\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Check the final parameters of the custom layer\n",
    "print(\"Custom layer weights:\", custom_layer.weight)\n",
    "print(\"Custom layer bias:\", custom_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the train_model function takes a model, input data, target, loss function, optimizer, and a list of additional checks as arguments. This makes the code more generic and allows you to easily apply the same training logic to different layers or models. The check_output_size, check_loss_value, and check_nan_parameters functions are examples of additional checks that can be applied during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4442346692085266\n",
      "Epoch 100, Loss: 0.3049066662788391\n",
      "Epoch 200, Loss: 0.2576864957809448\n",
      "Epoch 300, Loss: 0.23753777146339417\n",
      "Epoch 400, Loss: 0.2236039638519287\n",
      "Epoch 500, Loss: 0.2116963118314743\n",
      "Epoch 600, Loss: 0.20289433002471924\n",
      "Epoch 700, Loss: 0.19736634194850922\n",
      "Epoch 800, Loss: 0.19367939233779907\n",
      "Epoch 900, Loss: 0.1907043606042862\n",
      "Custom layer weights: Parameter containing:\n",
      "tensor([[ 0.5380,  0.7821,  0.2931,  0.3747,  0.1308],\n",
      "        [ 0.1686, -0.0026,  0.0795,  0.2055,  0.0280],\n",
      "        [ 0.4638,  0.6330,  0.1474,  0.2454,  0.6370],\n",
      "        [ 0.7677,  0.4780,  0.7362,  0.7217,  0.3613],\n",
      "        [ 0.0388,  0.1295,  0.8411,  0.8618,  0.0249],\n",
      "        [ 0.8336,  0.7655,  0.7907,  0.7028,  0.3246],\n",
      "        [ 0.6012,  0.2898,  0.7524,  0.6763,  0.7260],\n",
      "        [ 0.5308,  0.7941,  0.3912,  0.0096, -0.1027],\n",
      "        [ 0.6180,  0.2837,  0.3156,  0.5001, -0.1638],\n",
      "        [ 0.5713,  0.8290,  0.6917,  0.5471, -0.0986]], requires_grad=True)\n",
      "Custom layer bias: Parameter containing:\n",
      "tensor([-0.1798,  0.0762, -0.0606, -0.1989, -0.2751], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, input_data, target, loss_function, optimizer, epochs, additional_checks=None\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_data = input_data.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = model(input_data)\n",
    "\n",
    "        # Additional checks\n",
    "        if additional_checks is not None:\n",
    "            for check in additional_checks:\n",
    "                check(output, target, model)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Additional checks\n",
    "        optimizer.step()\n",
    "        if additional_checks is not None:\n",
    "            for check in additional_checks:\n",
    "                check(output, target, model)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Example usage for the generic training function with a custom layer\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x.mm(self.weight) + self.bias)\n",
    "\n",
    "\n",
    "# Additional check functions\n",
    "def check_output_size(output, target, model):\n",
    "    if not torch.is_tensor(output) or output.size() != target.size():\n",
    "        print(\"Warning: Unexpected forward pass output size.\")\n",
    "\n",
    "\n",
    "def check_loss_value(output, target, model):\n",
    "    loss_function = nn.MSELoss()\n",
    "    loss = loss_function(output, target)\n",
    "    if not torch.is_tensor(loss) or loss.dim() != 0:\n",
    "        print(\"Warning: Invalid loss value.\")\n",
    "\n",
    "\n",
    "def check_nan_parameters(output, target, model):\n",
    "    if any(torch.isnan(param).any() for param in model.parameters()):\n",
    "        print(\"Warning: NaN values detected in model parameters.\")\n",
    "\n",
    "\n",
    "# Example usage to train the custom layer with generic training function\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "custom_layer = CustomLayer(input_size, output_size)\n",
    "\n",
    "input_data = torch.rand((batch_size, input_size))\n",
    "target = torch.rand((batch_size, output_size))\n",
    "\n",
    "optimizer = optim.SGD(custom_layer.parameters(), lr=learning_rate)\n",
    "\n",
    "additional_checks = [check_output_size, check_loss_value, check_nan_parameters]\n",
    "\n",
    "train_model(\n",
    "    custom_layer, input_data, target, nn.MSELoss(), optimizer, epochs, additional_checks\n",
    ")\n",
    "\n",
    "# Check the final parameters of the custom layer\n",
    "print(\"Custom layer weights:\", custom_layer.weight)\n",
    "print(\"Custom layer bias:\", custom_layer.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
